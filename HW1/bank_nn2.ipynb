{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first GPU card\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Automatically grow GPU memory usage\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# Set the session used by Keras\n",
    "tf.keras.backend.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "\n",
    "    def __init__(self, path, train, test, result_name):\n",
    "        self.path = path\n",
    "        self.traindata = self.path+train\n",
    "        self.testdata = self.path+test\n",
    "        self.result_name = self.path+result_name\n",
    "        \n",
    "        self.to_int=0\n",
    "        # the columns I want to drop\n",
    "        self.d_col = ['day', 'month', 'pdays','previous']\n",
    "        \n",
    "    def read_csv(self, f):\n",
    "        data_list = pd.read_csv(f,sep=\",\")\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    # StandardScaler the data\n",
    "    def normalized(self, df):\n",
    "        \n",
    "        mean = np.mean(df, axis=0)\n",
    "        std = np.std(df, axis=0)\n",
    "        var = std * std\n",
    "\n",
    "        df_normalized = df - mean\n",
    "        df_normalized = df_normalized / std\n",
    "\n",
    "        return df_normalized\n",
    "\n",
    "\n",
    "    def convert_to_value(self, df):\n",
    "        # get the value type columns\n",
    "        value_cols = df.describe().columns\n",
    "        # get the word type columns\n",
    "        word_cols = list(set(df.columns)-set(value_cols))\n",
    "\n",
    "        # make a dictionary to every kind of type\n",
    "        # ,and change entire word type df to value type\n",
    "        for col in word_cols:\n",
    "            get_values = set(df[col].values)\n",
    "            category_dict = {v:i for i,v in enumerate(get_values)}        \n",
    "\n",
    "            arr = []\n",
    "            for value in df[col].values:\n",
    "                arr.append(category_dict[value])\n",
    "\n",
    "            df[col] = np.array(arr)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cross_validation_split(self, df):\n",
    "        # get the data and output\n",
    "        X = df.values[:, :-1]\n",
    "        Y = df.values[:,-1]\n",
    "\n",
    "        # random split data to 4 pieces, the test size is .25\n",
    "        # want to use cross validation\n",
    "        rs = ShuffleSplit(n_splits=10, test_size=.01)\n",
    "        rs_list = rs.split(X)      \n",
    "\n",
    "        return X, Y, rs_list\n",
    "\n",
    "\n",
    "    def label2int(self, ind):\n",
    "        \n",
    "        if ind==0:\n",
    "            self.to_int=0\n",
    "        elif ind==1:\n",
    "            self.to_int=1\n",
    "\n",
    "        return self.to_int\n",
    "\n",
    "\n",
    "    # make the y_label to onehot\n",
    "    def ylabel_to_onehot(self, Y):\n",
    "        onehotY = np.zeros((Y.shape[0],2))\n",
    "        for i in range(Y.shape[0]):\n",
    "            onehotY[i][self.label2int(Y[i])] = 1\n",
    "\n",
    "        return onehotY\n",
    "    \n",
    "    \n",
    "    # drop the columns I don't want\n",
    "    def drop_col(self, df):\n",
    "        df = df.drop(columns=self.d_col)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # map the ordered category\n",
    "    def process_ordered_list(self, df):\n",
    "        education_map = {'unknown':0,\n",
    "                         'primary':1,\n",
    "                         'secondary':2,\n",
    "                         'tertiary':3       \n",
    "        }\n",
    "        \n",
    "        df['education'] = df['education'].map(education_map)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # use onthot to process other category\n",
    "    def onehot_without_y(self, df):\n",
    "        onehot_cols = list(set(df.columns)-set(df.describe().columns)-set('y'))\n",
    "        onehotpd = pd.get_dummies(df[onehot_cols])\n",
    "        df = df.drop(columns=onehot_cols)\n",
    "        df = pd.concat([onehotpd, df], axis=1)\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(p_data, csv_df):\n",
    "    csv_df = p_data.drop_col(csv_df)\n",
    "    \n",
    "    nor_cols = csv_df.describe().columns\n",
    "    csv_df[nor_cols] = p_data.normalized(csv_df[nor_cols])\n",
    "    \n",
    "    csv_df = p_data.process_ordered_list(csv_df)\n",
    "    csv_df = p_data.onehot_without_y(csv_df)\n",
    "    \n",
    "    return csv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_trainset(csv_df):\n",
    "    X, Y, rs_list = p_data.cross_validation_split(csv_df)\n",
    "\n",
    "    # get the first fold split\n",
    "    rsl1,rsl2 = list(rs_list)[0]\n",
    "\n",
    "    onehotY = p_data.ylabel_to_onehot(Y)\n",
    "    \n",
    "    return X,onehotY,rsl1,rsl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_model(X,Y,*list_index):\n",
    "\n",
    "    li1,li2 = list_index\n",
    "    batch_size = 32\n",
    "    epochs = 1\n",
    "\n",
    "    bank_input = Input(shape=(X.shape[1],))\n",
    "    hidden = Dense(10, activation='relu', kernel_regularizer=l2(0.1),activity_regularizer=l2(0.1))(bank_input)\n",
    "#     dr = Dropout(0.4)(hidden)\n",
    "#     hidden = Dense(10, activation='relu', kernel_regularizer=l2(0.01),activity_regularizer=l2(0.01))(dr)\n",
    "#     hidden = Dense(256, activation='relu', kernel_regularizer=l2(0.01),activity_regularizer=l2(0.01))(hidden)\n",
    "#     hidden = Dense(128, activation='relu', kernel_regularizer=l2(0.01),activity_regularizer=l2(0.01))(hidden)\n",
    "    output = Dense(2, activation='softmax')(hidden)\n",
    "\n",
    "    model = Model(inputs=bank_input, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X[li1], Y[li1],\n",
    "             batch_size=batch_size,\n",
    "             epochs=epochs,\n",
    "             validation_data=(X[li2], Y[li2]),\n",
    "             )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testdata, p_data, model):\n",
    "    \n",
    "    test_df = pre_process(p_data, testdata)\n",
    "\n",
    "    y_prob = model.predict(test_df)\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    sub_csv = pd.DataFrame({'id':np.arange(y_classes.shape[0]),\n",
    "                            'ans':y_classes\n",
    "                           },\n",
    "                          columns=['id','ans'])\n",
    "\n",
    "    \n",
    "    sub_csv.to_csv(p_data.result_name, index=False)\n",
    "    \n",
    "    return y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                340       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 362\n",
      "Trainable params: 362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40282 samples, validate on 407 samples\n",
      "Epoch 1/1\n",
      "40282/40282 [==============================] - 9s 236us/step - loss: 0.8851 - acc: 0.9983 - val_loss: 0.2023 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "path = \"/dataset/Bank Marketing Data Set/\"\n",
    "train_data = \"training_data.csv\"\n",
    "test_data = \"testing_data.csv\"\n",
    "result_name = 'sub_res2.csv'\n",
    "\n",
    "p_data = PreProcessing(path, train_data, test_data, result_name)\n",
    "\n",
    "traindata = p_data.read_csv(p_data.traindata)\n",
    "csv_df = pre_process(p_data, traindata)\n",
    "X, onehotY, rsl1, rsl2 = spilt_trainset(csv_df)\n",
    "model = bulid_model(X,onehotY,rsl1,rsl2)\n",
    "\n",
    "testdata = p_data.read_csv(p_data.testdata)\n",
    "test_v = test(testdata,p_data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = '/dataset/Bank Marketing Data Set/evaluate.csv'\n",
    "\n",
    "evalution = pd.read_csv(res_path)\n",
    "eva_v = evalution['ans'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117647058823529"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = accuracy_score(test_v, eva_v)  \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
