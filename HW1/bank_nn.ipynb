{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 173,186\n",
      "Trainable params: 173,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30516 samples, validate on 10173 samples\n",
      "Epoch 1/30\n",
      "30516/30516 [==============================] - 8s 271us/step - loss: 0.0127 - acc: 0.9951 - val_loss: 1.2195e-05 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "30516/30516 [==============================] - 8s 256us/step - loss: 6.3452e-06 - acc: 1.0000 - val_loss: 3.1994e-06 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "30516/30516 [==============================] - 8s 264us/step - loss: 2.0588e-06 - acc: 1.0000 - val_loss: 1.2873e-06 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "30516/30516 [==============================] - 8s 272us/step - loss: 8.9989e-07 - acc: 1.0000 - val_loss: 6.0505e-07 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "30516/30516 [==============================] - 8s 269us/step - loss: 4.4801e-07 - acc: 1.0000 - val_loss: 3.3558e-07 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "30516/30516 [==============================] - 8s 269us/step - loss: 2.3977e-07 - acc: 1.0000 - val_loss: 1.4921e-07 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "30516/30516 [==============================] - 8s 257us/step - loss: 1.2645e-07 - acc: 1.0000 - val_loss: 1.0981e-07 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "30516/30516 [==============================] - 8s 260us/step - loss: 1.0963e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "30516/30516 [==============================] - 8s 250us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "30516/30516 [==============================] - 8s 253us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "30516/30516 [==============================] - 8s 259us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "30516/30516 [==============================] - 8s 257us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "30516/30516 [==============================] - 8s 258us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "30516/30516 [==============================] - 8s 253us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "30516/30516 [==============================] - 8s 258us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "30516/30516 [==============================] - 8s 252us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "30516/30516 [==============================] - 8s 249us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "30516/30516 [==============================] - 8s 250us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "30516/30516 [==============================] - 7s 243us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "30516/30516 [==============================] - 8s 262us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "30516/30516 [==============================] - 8s 259us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "30516/30516 [==============================] - 8s 252us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "30516/30516 [==============================] - 8s 260us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "30516/30516 [==============================] - 8s 256us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "30516/30516 [==============================] - 8s 247us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "30516/30516 [==============================] - 7s 242us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "30516/30516 [==============================] - 8s 253us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "30516/30516 [==============================] - 8s 251us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "30516/30516 [==============================] - 8s 252us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "30516/30516 [==============================] - 8s 255us/step - loss: 1.0960e-07 - acc: 1.0000 - val_loss: 1.0960e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   \n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n",
    "#=============GPU enviroment=============\n",
    "# Use the first GPU card\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Automatically grow GPU memory usage\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# Set the session used by Keras\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "#========================================\n",
    "\n",
    "class PreProcessing():\n",
    "\n",
    "    def __init__(self, path, train, test):\n",
    "        self.path = path\n",
    "        self.traindata = self.path+train\n",
    "        self.testdata = self.path+test\n",
    "        \n",
    "        self.to_int=0\n",
    "        \n",
    "    def read_csv(self, f):\n",
    "        data_list = pd.read_csv(f,sep=\",\")\n",
    "        return data_list\n",
    "\n",
    "\n",
    "    # StandardScaler the data\n",
    "    def normalized(self, df):\n",
    "        mean = np.mean(df, axis=0)\n",
    "        std = np.std(df, axis=0)\n",
    "        var = std * std\n",
    "\n",
    "        df_normalized = df - mean\n",
    "        df_normalized = df_normalized / std\n",
    "\n",
    "        return df_normalized\n",
    "\n",
    "\n",
    "    def convert_to_value(self, df):\n",
    "        # get the value type columns\n",
    "        value_cols = df.describe().columns\n",
    "        # get the word type columns\n",
    "        word_cols = list(set(df.columns)-set(value_cols))\n",
    "\n",
    "        # make a dictionary to every kind of type\n",
    "        # ,and change entire word type df to value type\n",
    "        for col in word_cols:\n",
    "            get_values = set(df[col].values)\n",
    "            category_dict = {v:i for i,v in enumerate(get_values)}        \n",
    "\n",
    "            arr = []\n",
    "            for value in df[col].values:\n",
    "                arr.append(category_dict[value])\n",
    "\n",
    "            df[col] = np.array(arr)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cross_validation_split(self, df):\n",
    "        # get the data and output\n",
    "        X = df.values[:, :-1]\n",
    "        Y = df.values[:,-1]\n",
    "\n",
    "        # random split data to 4 pieces, the test size is .25\n",
    "        # want to use cross validation\n",
    "        rs = ShuffleSplit(n_splits=4, test_size=.25)\n",
    "        rs_list = rs.split(X)      \n",
    "\n",
    "        return X, Y, rs_list\n",
    "\n",
    "\n",
    "    def label2int(self, ind):\n",
    "        \n",
    "        if ind==0:\n",
    "            self.to_int=0\n",
    "        elif ind==1:\n",
    "            self.to_int=1\n",
    "\n",
    "        return self.to_int\n",
    "\n",
    "\n",
    "    # make the y_label to onehot\n",
    "    def ylabel_to_onehot(self, Y):\n",
    "        onehotY = np.zeros((Y.shape[0],2))\n",
    "        for i in range(Y.shape[0]):\n",
    "            onehotY[i][self.label2int(Y[i])] = 1\n",
    "\n",
    "        return onehotY\n",
    "\n",
    "#===========build model=====================\n",
    "\n",
    "def bulid_model(X,Y,*list_index):\n",
    "\n",
    "    l1,l2 = list_index\n",
    "    batch_size = 32\n",
    "    epochs = 30\n",
    "\n",
    "    bank_input = Input(shape=(X.shape[1],))\n",
    "    hidden = Dense(512)(bank_input)\n",
    "    hidden = Dense(256)(hidden)\n",
    "    hidden = Dense(128)(hidden)\n",
    "    output = Dense(2, activation='softmax')(hidden)\n",
    "\n",
    "    model = Model(inputs=bank_input, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X[l1], Y[l1],\n",
    "             batch_size=batch_size,\n",
    "             epochs=epochs,\n",
    "             validation_data=(X[l2], Y[l2]),\n",
    "             )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#===================train====================\n",
    "def train_and_process(p_data):\n",
    "\n",
    "    csv_df = p_data.read_csv(p_data.traindata)\n",
    "    csv_df.iloc[:, :-1] = p_data.convert_to_value(csv_df.iloc[:, :-1])\n",
    "\n",
    "    # make X values normalized\n",
    "    csv_df.iloc[:, :-1] = p_data.normalized(csv_df.iloc[:, :-1])\n",
    "\n",
    "    # spilt the data to train set and validation set\n",
    "    # ,and get the list of split index\n",
    "    X, Y, rs_list = p_data.cross_validation_split(csv_df)\n",
    "\n",
    "    # get the first fold split\n",
    "    l1,l2 = list(rs_list)[0]\n",
    "\n",
    "    onehotY = p_data.ylabel_to_onehot(Y)\n",
    "    model = bulid_model(X,onehotY,l1,l2)\n",
    "\n",
    "    return model\n",
    "\n",
    "#===================test========================\n",
    "\n",
    "def test(p_data,model):\n",
    "\n",
    "    test_df = p_data.read_csv(p_data.testdata)\n",
    "    test_df = p_data.convert_to_value(test_df)\n",
    "    test_df = p_data.normalized(test_df)\n",
    "\n",
    "    y_prob = model.predict(test_df)\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    sub_csv = pd.DataFrame({'id':np.arange(y_classes.shape[0]),\n",
    "                            'ans':y_classes\n",
    "                           },\n",
    "                          columns=['id','ans'])\n",
    "\n",
    "    result_name = 'sub_res.csv'\n",
    "    sub_csv.to_csv(p_data.path+result_name, index=False)\n",
    "\n",
    "\n",
    "#================main======================\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    path = \"/dataset/Bank Marketing Data Set/\"\n",
    "    train_data = \"training_data.csv\"\n",
    "    test_data = \"testing_data.csv\"\n",
    "\n",
    "    p_data = PreProcessing(path, train_data, test_data)\n",
    "\n",
    "    model = train_and_process(p_data)\n",
    "    test(p_data,model)\n",
    "\n",
    "\n",
    "#========================================\n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
